---
displayed_sidbar: referenceSidebar
slug: /python/DataImport-LocalBulkWriter
beta: false
notebook: false
type: folder
token: SzykfW0E7lSK4sdsvhOcAhkan0b
sidebar_position: 3
---

import Admonition from '@theme/Admonition';


# LocalBulkWriter

A LocalBulkWriter instance rewrites your raw data locally in a format that Zilliz Cloud understands.

```python
class pymilvus.LocalBulkWriter
```

## Constructor{#constructor}

Constructs a LocalBulkWriter object by schema, output path, segment size, and file type.

<Admonition type="info" icon="ðŸ“˜" title="Notes">

<p>A <strong>LocalBulkWriter</strong> object intends to rewrite your raw data locally in a format that Zilliz Cloud understands.</p>

</Admonition>

```python
from pymilvus import CollectionSchema, LocalBulkWriter, BulkFileType

writer = LocalBulkWriter(
    schema=CollectionSchema(),
    local_path="string",
    segment_size=512*1024*1024,
    file_type=BulkFileType.NPY
)
```

__PARAMETERS:__

- __schema__ (_[CollectionSchema](./ORM-CollectionSchema)_) -

    __[REQUIRED]__

    The schema of a target collection to which the rewritten data is to be imported.

- __local_path__ (_str_) -

    __[REQUIRED]__

    The path to the directory that is to hold the rewritten data.

- __segment_size__ (_int_) -

    The maximum size of a file segment.

    While rewriting your raw data, Zilliz Cloud splits your raw data into segments.

    The value defaults to __536,870,912__ in bytes, which is __512__ MB.

    <Admonition type="info" icon="ðŸ“˜" title="How does BulkWriter segment my data?">

    <p>The way <strong>BulkWriter</strong> segments your data varies with the target file type.</p>
    <ul>
    <li><strong>JSON_RB</strong> or <strong>Parquet</strong></li>
    </ul>
    <p>If the generated file exceeds the specified segment size, <strong>BulkWriter</strong> creates multiple files and names them in sequence numbers, each no larger than the segment size.</p>
    <ul>
    <li><strong>NPY</strong></li>
    </ul>
    <p>If the generated file exceeds the specified segment size, <strong>BulkWriter</strong> creates multiple subdirectories and names them in sequence numbers. Each subdirectory contains all the necessary NumPy files that are no larger than the segment size.</p>

    </Admonition>

- __file_type__ (_[BulkFileType](./DataImport-BulkFileType)_) -

    The type of the output file.

    The value defaults to __BulkFileType.NPY__. 

    Possible options are __BulkFileType.NPY__, __BulkFileType.JSON_RB__ and __BulkFileType.PARQUET__.

__RETURN TYPE:__

_LocalBulkWriter_

__RETURNS:__

A __LocalBulkWriter__ object.

__EXCEPTIONS:__

- __SchemaNotReadyException__

    This exception will be raised when the provided schema is invalid.

## Properties{#properties}

- __uuid__ (_str_) -

    A randomly generated UUID, used to name the output file or directory, with JSON, Parquet, and NumPy formats supported.

- __data_path__ (_pathlib.PosixPath_) -

    The path to the output directory.

- __batch_files__ (_str_) -

    A list of the generated file names.

## Methods{#methods}

The following are the methods of the __LocalBulkWriter__ class:



import DocCardList from '@theme/DocCardList';

<DocCardList />