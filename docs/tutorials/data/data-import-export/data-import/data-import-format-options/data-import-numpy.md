---
title: "ä» NumPy æ–‡ä»¶ä¸­å¯¼å…¥ | Cloud"
slug: /data-import-numpy
sidebar_label: "ä» NumPy æ–‡ä»¶ä¸­å¯¼å…¥"
beta: FALSE
notebook: FALSE
description: "The `.npy` format is NumPy's standard binary format](https//numpy.org/devdocs/reference/generated/numpy.lib.format.html) for saving a single array, including its shape and dtype information, ensuring it can be correctly reconstructed on different machines.  You are advised to use [the BulkWriter tool to prepare your raw data into Parquet files. The following figure demonstrates how your raw data can be mapped into a set of `.npy` file. | Cloud"
type: origin
token: QyBEwkTplilK9LksUSjcP2k1nNf
sidebar_position: 3
keywords: 
  - å‘é‡æ•°æ®åº“
  - zilliz
  - milvus
  - å¤§æ¨¡å‹å‘é‡æ•°æ®åº“
  - æ•°æ®å¯¼å…¥
  - numpy
  - numpy files
  - import from numpy
  - ä» numpy æ–‡ä»¶å¯¼å…¥

---

import Admonition from '@theme/Admonition';


# ä» NumPy æ–‡ä»¶ä¸­å¯¼å…¥

The `.npy` format is [NumPy's standard binary format](https://numpy.org/devdocs/reference/generated/numpy.lib.format.html) for saving a single array, including its shape and dtype information, ensuring it can be correctly reconstructed on different machines.  You are advised to use [the BulkWriter tool](./use-bulkwriter) to prepare your raw data into Parquet files. The following figure demonstrates how your raw data can be mapped into a set of `.npy` file.

![data_import-preparetion_en](/img/data_import-preparetion_en.png)

<Admonition type="info" icon="ğŸ“˜" title="Notes">

<ul>
<li><strong>Whether to enable AutoID</strong></li>
</ul>
<p>The <strong>id</strong> field serves as the primary field of the collection. To make the primary field automatically increment, you can enable <strong>AutoID</strong> in the schema. In this case, you should exclude the <strong>id</strong> field from each row in the source data.</p>
<ul>
<li><strong>Whether to enable dynamic fields</strong></li>
</ul>
<p>When the target collection enables dynamic fields, if you need to store fields that are not included in the pre-defined schema, you can specify the <strong>$meta</strong> column during the write operation and provide the corresponding key-value data.</p>
<ul>
<li><strong>Case-sensitive</strong></li>
</ul>
<p>Dictionary keys and collection field names are case-sensitive. Ensure that the dictionary keys in your data exactly match the field names in the target collection. If there is a field named <strong>id</strong> in the target collection, each entity dictionary should have a key named <strong>id.</strong> Using <strong>ID</strong> or <strong>Id</strong> results in errors. </p>

</Admonition>

## Directory structure

To prepare your data as NumPy files, place all files from the same subset into a folder, then group these folders within the source folder, as shown in the tree diagram below.

```bash
â”œâ”€â”€ numpy-folders
â”‚       â”œâ”€â”€ 1
â”‚       â”‚   â”œâ”€â”€ id.npy
â”‚       â”‚   â”œâ”€â”€ vector.npy
â”‚       â”‚   â”œâ”€â”€ scalar_1.npy
â”‚       â”‚   â”œâ”€â”€ scalar_2.npy
â”‚       â”‚   â””â”€â”€ $meta.npy 
â”‚       â””â”€â”€ 2
â”‚           â”œâ”€â”€ id.npy
â”‚           â”œâ”€â”€ vector.npy
â”‚           â”œâ”€â”€ scalar_1.npy
â”‚           â”œâ”€â”€ scalar_2.npy
â”‚           â””â”€â”€ $meta.npy  
```

## Import data

Once your data is ready, you can use either of the following methods to import them into your Zilliz Cloud collection.

- [Import files from a list of NumPy files folders (recommended)](./data-import-numpy#import-files-from-a-list-of-numpy-file-folders-recommended)

- [Import files from a NumPy file folder](./data-import-numpy#import-files-from-a-numpy-file-folder)

<Admonition type="info" icon="ğŸ“˜" title="Notes">

<p>If your files are relatively small, it is recommended to use the folder or multiple-path method to import them all at once. This approach allows for internal optimizations during the import process, which helps reduce resource consumption later.</p>

</Admonition>

You can also import your data on the Zilliz Cloud console using Milvus SDKs. For details, refer to [é€šè¿‡ Web æ§åˆ¶å°å¯¼å…¥](./import-data-on-web-ui) and [é€šè¿‡ SDK å¯¼å…¥](./import-data-via-sdks).

### Import files from a list of NumPy file folders (Recommended)

When importing files from multiple paths, include each NumPy file folder path in a separate list, then group all the lists into a higher-level list as in the following code example.

```bash
curl --request POST \
     --url "https://api.cloud.zilliz.com/v2/vectordb/jobs/import/create" \
     --header "Authorization: Bearer ${TOKEN}" \
     --header "Accept: application/json" \
     --header "Content-Type: application/json" \
     -d '{
        "clusterId": "inxx-xxxxxxxxxxxxxxx",
        "collectionName": "medium_articles",
        "partitionName": "",
        "objectUrls": [
            ["s3://bucket-name/numpy-folder-1/1/"],
            ["s3://bucket-name/numpy-folder-2/1/"],
            ["s3://bucket-name/numpy-folder-3/1/"]
         ],
        "accessKey": "",
        "secretKey": ""
    }'
```

### Import files from a NumPy file folder

If the source folder contains only the NumPy file folder to import, you can simply include the source folder in the request as follows:

```bash
curl --request POST \
     --url "https://api.cloud.zilliz.com/v2/vectordb/jobs/import/create" \
     --header "Authorization: Bearer ${TOKEN}" \
     --header "Accept: application/json" \
     --header "Content-Type: application/json" \
     -d '{
        "clusterId": "inxx-xxxxxxxxxxxxxxx",
        "collectionName": "medium_articles",
        "partitionName": "",
        "objectUrls": [
            ["s3://bucket-name/numpy-folder/1/"]
         ],
        "accessKey": "",
        "secretKey": ""
    }'
```

## Storage paths

Zilliz Cloud supports data import from your cloud storage. The table below lists the possible storage paths for your data files.

<table>
   <tr>
     <th><p><strong>Cloud</strong></p></th>
     <th><p><strong>Quick Examples</strong></p></th>
   </tr>
   <tr>
     <td><p><strong>AWS S3</strong></p></td>
     <td><p><code>s3://bucket-name/numpy-folder/</code></p></td>
   </tr>
   <tr>
     <td><p><strong>Google Cloud Storage</strong></p></td>
     <td><p><code>gs://bucket-name/numpy-folder/</code></p></td>
   </tr>
   <tr>
     <td><p><strong>Azure Bolb</strong></p></td>
     <td><p><code>https://myaccount.blob.core.windows.net/bucket-name/numpy-folder/</code></p></td>
   </tr>
</table>

## Limits

A valid set of NumPy files should be named after the fields in the schema of the target collection, and the data in them should match the corresponding field definitions.

<table>
   <tr>
     <th><p><strong>Item</strong></p></th>
     <th><p><strong>Description</strong></p></th>
   </tr>
   <tr>
     <td><p><strong>Multiple files per import</strong></p></td>
     <td><p>Yes</p></td>
   </tr>
   <tr>
     <td><p><strong>Data import from first-level subfolders</strong></p></td>
     <td><p>Yes</p></td>
   </tr>
   <tr>
     <td><p><strong>Maximum number of first-level subfolders</strong></p></td>
     <td><p>100</p></td>
   </tr>
   <tr>
     <td><p><strong>Maximum file size per import</strong></p></td>
     <td><p>Free cluster: 512 MB in total</p><p>Serverless &amp; Dedicated cluster:</p><ul><li><p>Total file size in each first-level subfolder: 10 GB</p></li><li><p>Total file size: 100 GB</p></li></ul></td>
   </tr>
   <tr>
     <td><p><strong>Applicable data file locations</strong></p></td>
     <td><p>Remote files only</p></td>
   </tr>
</table>

You can use [the BulkWriter tool](./use-bulkwriter) to generate the source data file. [Click here to download the prepared sample data based on the schema in the above diagram](https://assets.zilliz.com/prepared_numpy_data.zip).